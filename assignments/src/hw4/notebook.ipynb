{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision onnx\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boats(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, gt_json_path=\"\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.gt_json_path = gt_json_path\n",
    "        self.labels = json.load(open(gt_json_path, \"r\"))\n",
    "        self.image_list = sorted(os.listdir(root_dir))\n",
    "        self.image_ids = dict(enumerate(self.image_list, start=0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.load_image(idx)\n",
    "        img_name = self.image_ids[idx]\n",
    "        label = self.labels[img_name]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        sample = (img, label)\n",
    "        return sample\n",
    "\n",
    "    def load_image(self, image_index):\n",
    "        image_name = self.image_ids[image_index]\n",
    "        path = os.path.join(self.root_dir, image_name)\n",
    "        img = imread(path)\n",
    "        return img\n",
    "\n",
    "\n",
    "# Improved Neural Network version\n",
    "class Net(nn.Module):#TODO 9) #creat your own NN architecture\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #self.fc1 = nn.Linear(3 * 192 * 108, 1)#TODO Question 1)\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(64 * 13 * 24, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional and Pooling Layers with Activation\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Output: (16, 54, 96)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Output: (32, 27, 48)\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # Output: (64, 13, 24)\n",
    "        \n",
    "        # Flatten\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # Fully Connected Layers with Activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Output Layer with Sigmoid Activation\n",
    "        output = torch.sigmoid(self.fc2(x))\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, criterion, epoch,dry_run):\n",
    "    \"\"\"\n",
    "    Train a network\n",
    "    You can find example code here: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device).float()\n",
    "        optimizer.zero_grad()#TODO Question 4)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, torch.unsqueeze(target, 1))#TODO Question 5)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device).float()\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, torch.unsqueeze(target, 1)).item()  # sum up batch loss\n",
    "            pred = torch.round(output)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3765 (0%)]\tLoss: 0.368959\n",
      "Train Epoch: 1 [640/3765 (17%)]\tLoss: 0.786761\n",
      "Train Epoch: 1 [1280/3765 (34%)]\tLoss: 0.343750\n",
      "Train Epoch: 1 [1920/3765 (51%)]\tLoss: 0.399792\n",
      "Train Epoch: 1 [2560/3765 (68%)]\tLoss: 0.575552\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Training settings #you can mess around with change these values!\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    epochs = 20\n",
    "    learning_rate = 0.0001\n",
    "    no_cuda = False #If you using course cpu leave False, if you are using GPU set true\n",
    "    dry_run = False\n",
    "    seed = random.randint(1,1000)#random seed. Set to constant if you want to train on the same data\n",
    "    log_interval = 10#how many batches to wait before logging training status\n",
    "    save_model = False \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #This is used if you want to run it as a script file.\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Ship Detection\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=64, metavar=\"N\",\n",
    "                        help=\"input batch size for training (default: 64)\")\n",
    "    parser.add_argument(\"--test-batch-size\", type=int, default=1000, metavar=\"N\",\n",
    "                        help=\"input batch size for testing (default: 1000)\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=14, metavar=\"N\",\n",
    "                        help=\"number of epochs to train (default: 14)\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.1, metavar=\"LR\",\n",
    "                        help=\"learning rate (default: 0.1)\")\n",
    "    parser.add_argument(\"--no-cuda\", action=\"store_true\", default=False,\n",
    "                        help=\"disables CUDA training\")\n",
    "    parser.add_argument(\"--dry-run\", action=\"store_true\", default=False,\n",
    "                        help=\"quickly check a single pass\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\",\n",
    "                        help=\"random seed (default: 1)\")\n",
    "    parser.add_argument(\"--log-interval\", type=int, default=10, metavar=\"N\",\n",
    "                        help=\"how many batches to wait before logging training status\")\n",
    "    parser.add_argument(\"--save-model\", action=\"store_true\", default=False,\n",
    "                        help=\"For Saving the current Model\")\n",
    "    args = parser.parse_args()\n",
    "    \"\"\"\n",
    "    #torch.manual_seed(args.seed)\n",
    "    torch.manual_seed(seed)\n",
    "    #use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    use_cuda = no_cuda\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    #train_kwargs = {\"batch_size\": args.batch_size}\n",
    "    #val_kwargs = {\"batch_size\": args.test_batch_size}\n",
    "    train_kwargs = {\"batch_size\": batch_size}\n",
    "    val_kwargs = {\"batch_size\": test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {\"num_workers\": 1,\n",
    "                       \"pin_memory\": True,\n",
    "                       \"shuffle\": True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        val_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    # Create transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # This normalization is used on the test server\n",
    "        transforms.Normalize([0.2404, 0.2967, 0.3563], [0.0547, 0.0527, 0.0477])\n",
    "        ])\n",
    "\n",
    "    # Create train and test set\n",
    "    path_to_dataset = \"/courses/CS5330.202510/data/Boat-MNIST\"#gobal path to the data on Discovery \n",
    "    train_set = Boats(root_dir=path_to_dataset + \"/train\", transform=transform,\n",
    "                      gt_json_path=path_to_dataset + \"/boat_mnist_labels_trainval.json\")\n",
    "    val_set = Boats(root_dir=path_to_dataset + \"/val\", transform=transform,\n",
    "                    gt_json_path=path_to_dataset +\"/boat_mnist_labels_trainval.json\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(val_set, **val_kwargs)\n",
    "\n",
    "    # Create network, optimizer and loss\n",
    "    model = Net().to(device)#TODO Question 6)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)#TODO Question 7)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()#TODO Question 8)\n",
    "\n",
    "    # Train and validate\n",
    "    best_acc = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(log_interval, model, device, train_loader, optimizer, criterion, epoch, dry_run)\n",
    "        acc = test(model, device, test_loader, criterion)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f\"Best accuracy (val): {best_acc}\")\n",
    "\n",
    "    #if args.save_model:\n",
    "    #    torch.save(model.state_dict(), \"model.pth\")\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "    \n",
    "    # --- Do not touch -----\n",
    "    # Save model as onnx file\n",
    "    dummy_input = torch.randn(1, 3, 108, 192, device=device)\n",
    "    input_names = [\"img_1\"]\n",
    "    output_names = [\"output1\"]\n",
    "    torch.onnx.export(model, dummy_input, \"ship_example.onnx\", input_names=input_names, output_names=output_names)\n",
    "    # ----------------------\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to questions:\n",
    "- **Q1:** The line `self.fc1 = nn.Linear(3 * 192 * 108, 1)` defines a fully connected layer that takes an input size of `3 * 192 * 108`, which represents the total number of pixels in a flattened RGB image of size `192x108. The output size is 1, producing a single value that can be used for binary classification (e.g., predicting if an image contains a boat or not).\n",
    "\n",
    "- **Q2:** The line `x = torch.flatten(x, start_dim=1)` reshapes the tensor `x` into a 2D tensor by flattening all dimensions starting from dimension 1 (keeping the batch size dimension intact). This is necessary to convert the multi-dimensional output from convolutional layers into a flat vector that can be input into a fully connected layer.\n",
    "\n",
    "- **Q3:** The line `x = self.fc1(x)` passes the flattened input `x` through the fully connected layer `fc1`. This layer applies a linear transformation to the input data, producing an output that can be further processed or used for predictions.\n",
    "\n",
    "- **Q4:** The line `optimizer.zero_grad()` clears the gradients of all optimized tensors. This is important because gradients are accumulated by default in PyTorch, so we need to zero them out at the start of each training iteration to prevent accumulation from previous iterations.\n",
    "\n",
    "- **Q5:** The line `loss = criterion(output, torch.unsqueeze(target, 1))` computes the loss between the model's predicted output and the actual target values. The `torch.unsqueeze(target, 1)` function adds an extra dimension to the target tensor to match the shape of the output tensor, as the loss function expects both inputs to have the same shape.\n",
    "\n",
    "- **Q6:** The line `model = Net().to(device)` initializes the neural network model and moves it to the specified device (`device`), which can be either a CPU or GPU. This ensures that all model computations are performed on the chosen hardware.\n",
    "\n",
    "- **Q7:** The line `optimizer = optim.SGD(model.parameters(), lr=learning_rate)` creates an optimizer called \"SGD\" (Stochastic Gradient Descent) to adjust the parameters of the model (\"model.parameters()\") during training, with a specified \"learning_rate\" which controls how much the model updates its parameters in each training step; essentially, it sets up the mechanism to optimize the model using the SGD algorithm with a defined learning rate.\n",
    "\n",
    "- **Q8:** The line `criterion = nn.MSELoss()` defines a variable that will be used to calculate the Mean Squared Error (MSE) loss between the predicted values of the neural network and the actual target values; essentially, it sets up a loss function that measures how far off predictions are from the ground truth on average, by squaring the difference between them and then taking the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Neural Network Architecture\n",
    "\n",
    "**Convolutional Layers:**\n",
    "\n",
    "- **`conv1` Layer:**\n",
    "  - Input Channels: 3 (RGB image)\n",
    "  - Output Channels: 16\n",
    "  - Kernel Size: 5x5\n",
    "  - Purpose: Extract low-level features like edges and textures.\n",
    "\n",
    "- **`conv2` Layer:**\n",
    "  - Input Channels: 16\n",
    "  - Output Channels: 32\n",
    "  - Kernel Size: 5x5\n",
    "  - Purpose: Capture more complex patterns by building upon features from `conv1`.\n",
    "\n",
    "- **`conv3` Layer:**\n",
    "  - Input Channels: 32\n",
    "  - Output Channels: 64\n",
    "  - Kernel Size: 3x3\n",
    "  - Purpose: Further abstract features and detect higher-level representations.\n",
    "\n",
    "**Pooling Layer:**\n",
    "\n",
    "- **MaxPool2d with Kernel Size 2x2:**\n",
    "  - Reduces the spatial dimensions by half each time it's applied.\n",
    "  - Helps in reducing computational load and controls overfitting.\n",
    "\n",
    "**Activation Function:**\n",
    "\n",
    "- **ReLU (Rectified Linear Unit):**\n",
    "  - Introduced after each convolutional layer.\n",
    "  - Adds non-linearity to the model, enabling it to learn complex patterns.\n",
    "\n",
    "**Fully Connected Layers:**\n",
    "\n",
    "- **`fc1` Layer:**\n",
    "  - Input Features: `64 * 13 * 24` (after flattening the output from `conv3`)\n",
    "  - Output Features: 128\n",
    "  - Purpose: Acts as a classifier on the features extracted by the convolutional layers.\n",
    "\n",
    "- **`fc2` Layer:**\n",
    "  - Input Features: 128\n",
    "  - Output Features: 1\n",
    "  - Purpose: Produces the final output score for binary classification.\n",
    "\n",
    "**Output Layer:**\n",
    "\n",
    "- **Sigmoid Activation Function:**\n",
    "  - Applied to the output of `fc2`.\n",
    "  - Converts the output score to a probability between 0 and 1.\n",
    "\n",
    "**Loss Function:**\n",
    "\n",
    "- **Binary Cross Entropy Loss (`nn.BCELoss`):**\n",
    "  - Suitable for binary classification tasks.\n",
    "  - Measures the difference between the predicted probabilities and the actual labels.\n",
    "\n",
    "**Optimizer:**\n",
    "\n",
    "- **Adam Optimizer:**\n",
    "  - An adaptive learning rate method.\n",
    "  - Combines the benefits of AdaGrad and RMSProp.\n",
    "  - Generally provides better performance compared to basic SGD.\n",
    "\n",
    "**Why This Architecture Improves Accuracy:**\n",
    "\n",
    "- **Feature Extraction:**\n",
    "  - Convolutional layers are effective at extracting hierarchical features from images.\n",
    "  - By stacking multiple convolutional layers, the network can learn both simple and complex patterns.\n",
    "\n",
    "- **Dimensionality Reduction:**\n",
    "  - Pooling layers reduce the spatial dimensions, making the computations more efficient and reducing the risk of overfitting.\n",
    "\n",
    "- **Non-Linearity:**\n",
    "  - ReLU activation functions introduce non-linearity, enabling the network to learn complex mappings between inputs and outputs.\n",
    "\n",
    "- **Classification Power:**\n",
    "  - Fully connected layers interpret the features extracted by convolutional layers and make predictions.\n",
    "\n",
    "- **Optimization:**\n",
    "  - Using the Adam optimizer helps in faster and more reliable convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
